# Educational-Platform-for-Pre-Primary-Students-Image-Processing-Machine-Learning-Augmented-Reality
Educational Platform for Pre-Primary Students: Image Processing, Machine Learning, Augmented Reality-BE IT project

What does the project do?
Phase 1: LEARNING 
We will create a book with printed alphabets and ask the kids to scan them using our platform. According to the letter scanned, an object would be popped up, using the technology of augmented reality. Using this close-to-reality and interactive experience, pre-primary students would learn the alphabets more enthusiastically. For example, if the letter ‘A’ is pointed at by the camera then the image of an ‘Apple’ is popped up using AR technology. 
Phase 2: TESTING 
Once the learning part of the alphabets is done with, our platform moves on to the next level. In this phase, our platform asks the student to recognize an object for the letter that we pop up on the screen, using AR again. For example, the student is asked to find an apple for the letter A. The student, in that case, searches his or her house for an apple and scans it. This image of the apple is pre-processed and using various machine learning algorithms, gives an output, stating how the recognition went. The result is displayed to the student using AR once again, and makes the student aware of much he or she knows.  
 
APPLICATIONS:  • For teaching pre-primary level kids in school. • Learning alphabets and revising them at home or just anywhere. • On increasing the database, this platform can be used to teach students till a PhD level as well. 

Why is it useful?
	All we sail through the 21st century, technology in the classroom or at home is becoming more and more predominant. The use of smartphones for any purpose is increasing day by day. Tablets are replacing our textbooks, and blackboards are replaced by digital blackboards. Now Education is more about visualization and practical knowledge rather than the theoretical one. This widespread adoption of technology has completely changed how teachers teach and students learn. Students are using advanced technology to shape how they learn. As mobile is handy we can use it anywhere anytime, so many business units in many industries have preferred to develop applications on the mobile platform.
Our brains make vision seem easy. It isn’t difficult for a human being to differentiate between a tiger and a zebra or between different faces. But when it comes to the computer, it is really hard for a machine to differentiate between objects. The field of machine learning has made tremendous progress in solving or addressing such difficult problems. A new model is introduced called as Convolutional Neural Networks (CNN) which achieves better performance. Augmented Reality [1] [10] is another new technology which can be implemented well in Educational fields for improving imagination and visualization of the students. AR technology has an ability to render objects that are hard to imagine and turn them into 3D models, thus making it easier to grasp the abstract and difficult content.
If you ask any parent of a 3-year-old or a teacher teaching in pre-primary as to how difficult it is to make small children sit at one place and make them study, you’ll mostly get answers saying the task is particularly difficult. Also, who can you blame; kids are so interesting in themselves that studying from a 2D book while sitting still is just too boring for them.  
Here is where our educational platform for children will come in handy! The basic idea here is to design a book ourselves for starters, having the letters from the alphabet written out. Then using our application on their parent’s cell phones, all the kids have to do is point the phone’s camera towards any alphabet and voila, a pop up image is seen on top of this letter corresponding to it. For example, if the letter ‘A’ is pointed at by the camera then the image of an ‘Apple’ is popped up using AR technology. 
The purpose behind making this application would be served as pre-primary students will now find it really interesting to study and learn much more than before in a fun way. 
“Alphabetter” is an application we developed which will help the pre-primary students to learn in a more exciting way. We focused on developing a real-time recognition of objects and 3d pop of those objects up to help visualize the child more about the specific object
How can users get started with the project?
A. Detection Method
Unity3D [5] is a powerful suite of tools (Project IDE, Code IDE, run-time) for AR. Firstly the target image for each alphabet is stored in the database called Vuforia [6] which submits it to Vuforia's Target Manager, making it recognizable by the system.  The target image is nothing but a marker which differentiates each alphabet from other. In the background, Vuforia creates an arrangement of the image using its features, and then the algorithm can find such patterns and track the targets. Basically, a feature in an image is a sharpened angle, for example: a box corner or the tip of a star. We have created the AR book which have the target image for each alphabet. We have taken the alphabets in upper case and lower case as the target images.  If you scan the target image the 3d object will be augmented on the screen. When the mobile camera scans the target image then the target image is searched in the Vuforia [6] database. 
B. Augmentation Method
The second process is augmentation method. First of all, the application will capture the frames from video and scans the target image. This target image will be compared with the target images in the Vuforia database. Now if the target image matches then the respective 3d object is augmented on the mobile screen with the respective audio. E.g.: “A for Apple” will be played at certain intervals till the mobile camera is scanning the target image of alphabet “A”. 
C. Process
1.	Firstly, a set of images are used to teach the network about the new classes to be recognized.
2.	Then the first phase analyzes all the images on disk and calculates the bottleneck values for each of them.
3.	Bottleneck is penultimate layer which has been trained to output a set of values that’s good enough for the classifier to use to distinguish between all the classes it’s been asked to recognize.
4.	Once the bottlenecks are complete, the actual training of the top layer of the network begins
5.	You'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy.
6.	The training accuracy shows what percent of the images used in the current training batch were labelled with the correct class. The validation accuracy is the precision on a randomly-selected group of images from a different set
7.	Cross entropy is a loss function which gives a glimpse into how well the learning process is progressing.
8.	By default, this script will run 4,000 training steps.
9.	An accuracy value of between 90% and 95%, though the exact value will vary from run to run since there's randomness in the training process. 
10.	This number is based on the percent of the images in the test set that are given the correct label after the model is fully trained.
11.	The script consists of TensorBoard [7] Summaries that makes it easy to understand, debug and optimize the retraining.
12.	Finally, the test data can be tested.
13.	D. Finding Pattern
14.	We start by creating a Graph Builder, we use to specify a model to run or load. We then start creating nodes for the small model we want to run to load, resize, and scale the pixel values to get the result the main model expects as its input. The first node we create is just a Constant that holds a tensor with the file name of the image we want to load. Now the system tries to figure out what kind of file it is and decode it. Then the system will keep adding more nodes to decode the file as an image, to cast the integers into floating point values, to resize the image and finally run operations on the pixel value. Then the system keeps adding more nodes, to decode the file data as an image, to cast the integers into floating point values, to resize it, and then finally to run the operations on the pixel values. At the end of this we have a model definition stored, which we turn into a full graph definition. Then we create an interface to actually running the graph, and run it, specifying which node we want to get the output from, and where to put the output data.
15.	This gives us a vector of Tensor objects. You can think of a Tensor as a multi-dimensional array in this context, and it holds a 224-pixel high, 224 pixels wide, and 3 channel image as float values, image size should be according to the model that we have chosen for our model we have chosen MobileNet.
16.	
17.	E. Pattern Matching.
18.	As the TensorFlow model uses graph and a label file as its database. The patterns are stored in this graph file which the application use for pattern matching and the label file contains the names of the object which the model is trained for. The graph file is stored in .pb extension i.e. it is a binary file and the label file is stored in the text format. The graph is stored in the binary file so that no changes can be made to it once the graph is created. The first graph which is created requires more memory and is not compatible for mobile platform. Hence, we need to optimize and quantize the graph created. The graph is then optimized first and then it is quantized. Then for pattern matching the main graph i.e. the quantized graph is loaded with the labels and the apk is built. Then the image from disk as a float array of numbers, resized and normalized to the specifications the main graph are expected. A sorted list of the highest-scoring labels is created. And then runs the short graph to get a pair of output tensors. In this case they represent the sorted scores and index positions of the highest results. The Print function takes those sorted results, and prints them out in a friendly way. As the model which accepts the images of size 224 x 224 then we Load, resize, and process i.e. actually run the image through the model. As we detect the objects at runtime streams of frames are given to the model. The video is converted to images/frames per sec and then the images are given to the model. Then the graph matches the pattern with the input image. Then we compare that the pattern of the object the user selected for detection and the pattern of the live image is almost same. And if they are then we print on the screen “Correctly found” else we display “Wrong”.
Sources to get help?
[1] Peng Chen, Xiaolin Liu, Wei Cheng, and Ronghuai Huang, “A review of using Augmented Reality in Education from 2011 to 2016”, Springer Science+Business Media Singapore, 2017. 
[2] Tharrenos Bratitsis, Pinelopi Bardanika, Michalis Ioannou, “Science Education and Augmented Reality content: The case of the Water Circle”, IEEE 17th International Conference on Advanced Learning Technologies, 2017. 
[3] Haifa Alhumaidan, Kathy Pui Ying Lo, Andrew Selby, “Co-Design of Augmented Reality Book for Collaborative Learning Experience in Primary Education”, SAI Intelligent Systems Conference | London, UK,  2015. 
[4] Ramsha Fatima, Iffat Zarrin, Mohammed A. Qadeer, M. Sarosh Umar, “Mobile Travel Guide using Image Recognition and GPS/Geo Tagging”, IEEE, 2016. 
[5] Poonsri Vate-U-Lan, “An Augmented Reality 3D Pop-Up Book: The Development of a Multimedia Project for English Language Teaching”, IEEE International Conference on Multimedia and Expo, 2012. 
[6] Orhan Yaman, Mehmet Karakose, “Development of Image Processing Based Methods Using Augmented Reality in Higher Education”, IEEE, 2016.
[7] Rubina Freitas, Pedro Campos, “SMART: A SysteM of Augmented Reality for Teaching 2nd Grade Students”, the British Computer Society, 2008.

